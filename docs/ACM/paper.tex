%% For double-blind review submission
\documentclass[sigplan,10pt,review,anonymous]{acmart}
\settopmatter{printfolios=true}
%% For final camera-ready submission
%\documentclass[acmlarge]{acmart}\settopmatter{}

\makeatletter\if@ACM@journal\makeatother

%% Journal information (used by PACMPL format)
%% Supplied to authors by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{1}
\acmArticle{1}
\acmYear{2017}
\acmMonth{1}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\else\makeatother

%% Conference information (used by SIGPLAN proceedings format)
%% Supplied to authors by publisher for camera-ready submission
\acmConference[PL'17]{IEEE}{2018}{Planet Earh}
\acmYear{2017}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\fi

\newcommand{\fer}[1]{\textcolor{red}{#1}}

%% For review submission
\setcopyright{none}

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
\citestyle{acmauthoryear}

%% Packages that we are using in this version of the work:
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{paralist}

% To turn comments OFF simply comment out the \Commentstrue line
\newif\ifComments\Commentstrue

\ifComments
\newcommand{\marcio}[1]{\noindent\textcolor{violet}{Marcio: {#1}}}
\newcommand{\guido}[1]{\noindent\textcolor{magenta}{Guido: {#1}}}
\newcommand{\fernando}[1]{\noindent\textcolor{brown}{Fernando: {#1}}}
\newcommand{\cesar}[1]{\noindent\textcolor{magenta}{Cesar: {#1}}}
\newcommand{\pedro}[1]{\noindent\textcolor{brown}{Pedro: {#1}}}
\newcommand{\rmv}[1]{\noindent\textcolor{gray}{Removed: {#1}}}
\newcommand{\new}[1]{\noindent\textcolor{blue}{ {#1}}}
\newcommand{\ed}[1]{\noindent\textcolor{red}{ {#1}}}
\else
\newcommand{\marcio}[1]{}
\newcommand{\guido}[1]{}
\newcommand{\fernando}[1]{}
\newcommand{\cesar}[1]{}
\newcommand{\pedro}[1]{}
\newcommand{\rmv}[1]{}
\newcommand{\new}[1]{#1}
\newcommand{\ed}[1]{}
\fi
\newcommand\dawn{\mbox{\textsf{DawnCC}}}
\newcommand\Taskminer{\mbox{\textsf{TaskMiner}}}

\newtheorem{Challenge}{Challenge}[section]

\begin{document}

\title[Automatic Identification and Annotation of Tasks in Structured
Programs]
{Automatic Identification and Annotation of Tasks in Structured Programs}

\author{Pedro Henrique Ramos Costa}
\authornote{with author1 note}          %% \authornote is optional;
\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Researcher}
  \department{DCC}
  \institution{UFMG}
  \streetaddress{6627 Antonio Carlos Avenue}
  \city{Belo Horizonte}
  \state{Minas Gerais}
  \postcode{31.270-213}
  \country{Brazil}
}
\email{pedroramos@dcc.ufmg.br}

\author{Gleison Souza Diniz Mendonc\c{c}a}
\authornote{with author1 note}          %% \authornote is optional;
\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Researcher}
  \department{DCC}
  \institution{UFMG}
  \streetaddress{6627 Antonio Carlos Avenue}
  \city{Belo Horizonte}
  \state{Minas Gerais}
  \postcode{31.270-213}
  \country{Brazil}
}
\email{gleison.mendonca@dcc.ufmg.br}

\author{Divino C\'{e}sar}
\authornote{with author1 note}
\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Researcher}
  \department{IC}
  \institution{UNICAMP}
  \streetaddress{Cidade Universit\'{a}ria}
  \city{Campinas}
  \state{S\~{a}o Paulo}
  \postcode{13083-970}
  \country{Brazil}
}
\email{divcesar@gmail.com}

\author{Guido Ara\'{u}jo}
\authornote{with author1 note}
\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Professor}
  \department{IC}
  \institution{UNICAMP}
  \streetaddress{Cidade Universit\'{a}ria}
  \city{Campinas}
  \state{S\~{a}o Paulo}
  \postcode{13083-970}
  \country{Brazil}
}
\email{guido@ic.unicamp.br}

\author{Fernando Magno Quint\~{a}o Pereira}
\authornote{with author1 note}
\orcid{nnnn-nnnn-nnnn-nnnn}
\affiliation{
  \position{Professor}
  \department{DCC}
  \institution{UFMG}
  \streetaddress{6627 Antonio Carlos Avenue}
  \city{Belo Horizonte}
  \state{Minas Gerais}
  \postcode{31.270-213}
  \country{Brazil}
}
\email{fernando@dcc.ufmg.br}          %% \email is recommended

\begin{abstract}
This paper reports the design and implementation of a suit of static analyses
and code generation techniques to annotate programs with OpenMP pragmas for
task parallelism.
These techniques deal with problems such as the discovery and use of program
symbols to estimate the profit of tasks, bound their recursive depth and limit
the memory regions onto which they operate.
These techniques have been implemented into the first source-to-source
compiler able to insert OpenMP pragmas into C/C++ programs without human
intervention.
By building onto the solid collection of static analyses available in LLVM,
and relying on OpenMP's runtime ability to disambiguate pointers, we show that
we can annotate large and convoluted programs, often replicating the performance
gains of handmade annotation.
\end{abstract}

%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011041</concept_id>
<concept_desc>Software and its engineering~Compilers</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011024.10011035</concept_id>
<concept_desc>Software and its engineering~Procedures, functions and subroutines</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003766.10003776</concept_id>
<concept_desc>Theory of computation~Regular languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10010124.10010131.10010134</concept_id>
<concept_desc>Theory of computation~Operational semantics</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Compilers}
\ccsdesc[300]{Software and its engineering~Procedures, functions and subroutines}
\ccsdesc[300]{Theory of computation~Regular languages}
\ccsdesc[300]{Theory of computation~Operational semantics}

\keywords{Parallelism, Tasks, OpenMP}

\maketitle

\section{Introduction}
\label{sec:intro}

Annotation systems have risen to a place of prominence as a simple and
effective means to write parallel programs.
Examples of such systems include OpenMP~\cite{JaegerCP15},
OpenACC~\cite{OpenACC20}, OpenHMPP~\cite{Andion14}, OpenMPC~\cite{Lee10},
OpenSs~\cite{MeenderinckJ11}, Cilk++~\cite{Leiserson09} and
Tareador~\cite{Ayguade15}.
Annotations work as a meta-language: they let developers grant parallel
semantics to syntax originally written to execute sequentially.
%\guido{What do you mean by modern accelerators? Not clear.}
Combined with modern hardware accelerators such as GPUs and FPGAs, they have led
to substantial performance gains~\cite{Bertolli14,Mendonca17,Poesia17,Reyes12,Wienke12}.
%\guido{Suppport what?}
Nevertheless, although convenient, the use of annotations is not straightforward, 
and still lacks supporting tools that help programmers to check if annotations
are correct and/or effective.
%replaced:
%Yet, even if convenient,

%Annotations such as OpenMP or OpenACC still require developers to
%worry about typical hassles of the parallel world, such as race conditions
%and deadlocks.
%Moreover, because they are used in tandem with imperative programming languages,
%such troubles are only made worse by pointer aliasing.
There exist tools that insert automatic annotations in
programs~\cite{Amini12,Guelton12,Mendonca16,Pingali11,Nugteren14}.
All these technologies explore data-parallelism -- the possibility of running
the same computation independently on different data.
Another form of parallelism, based on tasks, remains still uncharted land in what
concerns automatic annotation.
Such fact is unfortunate, since much of the power of current annotation
systems lays on their ability to create tasks~\cite{Ayguade09}.
%\guido{Why the ability to create tasks makes such programs similar to irregular code?
%Which features of irregular code are resposible for such similarities? Not clear to the 
%reader}
As we illustrate in Section~\ref{sub:adv}, task parallelism -- the power to run
different routines simultaneously on independent data -- brings annotation systems
closer to irregular programs such as those that process graphs and
worklists~\cite{Ben-Nun17,Kulkarni11,Pingali11}.
The purpose of this work is to address this omission.

This paper describes \Taskminer, a source-to-source compiler that exposes task
parallelism in C/C++ programs.
To fulfil this goal, \Taskminer{} solves different challenges.
First, it finds program regions, e.g., loops or functions, that can be
effectively mapped onto tasks (Section~\ref{sub:identification}).
Second, it determines symbolic bounds to the memory blocks accessed within
those regions (Section~\ref{sub:symb}).
Third, it extracts parameters from the code to estimate when it is profitable to
create tasks.
These parameters feed conditional checks, which, at runtime, enable or disable
the creation of tasks (Section~\ref{sub:profit}) and limit their recursion
depth (Section~\ref{sub:rec}).
Fourth, \Taskminer{} determines which program variables need to be 
privatized in new tasks, or shared among them (Section~\ref{sub:variance}).
Finally, it maps all this information back into source code, producing
readable annotations (Section~\ref{sub:ir}).

In this paper, we defend the thesis that automatic task annotations are
effective and useful.
Our techniques enhance the productivity of developers, because they save them
the time to annotate programs.
\Taskminer{} receives as input C code, and produces, as output, a C
program annotated with human-readable OpenMP task directives.
We are currently able to annotate non-trivial programs, involving every
sort of composite type available in the C language, e.g., arrays, structs,
unions and pointers to these aggregates.
Some of these programs, such as those taken from the
Kastor~\cite{Virouleau14} or Bots~\cite{Duran09}
benchmark suites, are large and complex; thus, their manual annotation is a
time consuming and error prone task.
Yet, our automatically annotated programs not only approximate the execution
times of the parallel versions of those benchmarks, but are much faster than
\guido{Numbers?}
their sequential --unannotated-- versions, as we report in Section~\ref{sec:eval}.

\section{Challenges}
\label{sec:ovf}

This paper presents a tool that annotates source C code with OpenMP task
directives.
This tool uses novel techniques to fulfill its purpose.
Such techniques solve four challenges, which Section~\ref{sub:dif}
illustrates with examples.
However, before we dive into these challenges, the reader must understand
the main benefits of using a runtime environment such as OpenMP's, for
such advantages have guided most of our designing decisions.
Section~\ref{sub:adv} introduces this discussion.

\subsection{The OpenMP Runtime System}
\label{sub:adv}

The main advantage of using OpenMP annotations to split a program into parallel
tasks is
the ability  of leaving the job of  handling dependences to a runtime.
The OpenMP runtime maintains a task dependence graph which dynamically exposes more parallelism 
opportunities than those resulting from a conservative  compile-time  analysis.
In particular, the runtime lets us circumvent the shortcomings that pointer aliasing
impose on the automatic parallelization of code.
Aliasing -- the possibility of two pointers dereferencing the same
memory region -- either prevents parallelism altogether, or forces compilers to
resort to complex runtime checks to ensure its correctness~\cite{Alves15,Rus02}.
The OpenMP runtime shields us from this problem, because
it already checks for dependencies among tasks, and dispatches them in a
correct order~\cite{LaGrone11}.

Dependence tracking can be complex, involving checks over ranges of addresses
(if the runtime system treats dependencies across memory ranges).
It can also include memory labelling and renaming (if the
runtime system is able to remove false dependencies).
Regardless of its capacity -- which is not the same across every OpenMP
implementation -- the runtime system will represent dependencies using a task
dependence graph (TDG)~\cite{Duran08}.
In this directed acyclic graph, nodes denote tasks and edges represent
dependences between them.
Tasks are dispatched for execution according to a dynamic topological ordering of
this graph~\cite{Planas15}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1\columnwidth]{images/ex_book_filter}
\caption{The benefits of the OpenMP runtime environment.
Neither the runtime checks of Alves~\cite{Alves15} or Rus~\cite{Rus02}, nor
Whaley's context and flow sensitive alias analysis would be able to ensure the
correctness of the automatic parallelization of this program.}
\label{fig:ex_book_filter}
\end{center}
\end{figure}

The OpenMP runtime support allows the parallelization of irregular applications,
such as programs that traverse data-structures formed by a mesh of pointers.
In such programs, control constructs like {\tt if} statements make the
execution  of certain pieces of code dependent on the program's input.
The runtime can capture such dependences, in contrast to static analysis tools.
As an example, Figure~\ref{fig:ex_book_filter} shows an application that
finds patterns in lines of a book.
The book is given as an array of pointers; each pointer leads to a
string representing a --potentially-- different line.
The natural parallelization of this program consists in firing off a task
to process each line.
Figure~\ref{fig:ex_book_filter} has been annotated automatically by
\Taskminer.
The OpenMP execution environment ensures the correct scheduling of the tasks
created at lines 9 and 11, by assuring that the annotated input dependencies
{\tt line} and {\tt pattern}  are 
respected at runtime.

\subsection{Challenges}
\label{sub:dif}

As seen in Section~\ref{sub:adv}, the OpenMP runtime liberates us
from the burden of having to track dependences between pointers statically.
However, the automatic insertion of effective task annotations into
programs still required us to deal with a number of challenges.
If left unsolved, these challenges would restrict our interventions
to trivial annotations -- hardly be of any use.
Our first challenge is inherent to any automatic parallelization system.

\begin{Challenge}
\label{ch:Regions}
Identify the memory region covered by a task.
\end{Challenge}

\begin{figure}[b!]
\begin{center}
\includegraphics[width=1\columnwidth]{images/ex_Regions}
\caption{Challenges~\ref{ch:Regions} and~\ref{ch:cost}: identifying memory regions
with symbolic limits, and using runtime information to estimate the profit of
tasks.}
\label{fig:ex_Regions}
\end{center}
\end{figure}

Figure~\ref{fig:ex_Regions} illustrates Challenge~\ref{ch:Regions}, and shows
how we solve it\footnote{Whenever we show code, statements in black font are part
of the original program.
The annotations that we create automatically appear in grey.}.
That program receives an $\mathsf{M}\times\mathsf{N}$ matrix \textsf{V}, in
linearized format, and produces a vector \textsf{U}, so that \textsf{U[i]}
contains the sum of all the elements in line \textsf{i} of matrix \textsf{V}.
For reasons to be considered in Section~\ref{sub:identification},
our static analysis determines that each iteration of the outermost loops
could be made into a task.
Thus, tasks comprise the innermost loop, and traverse the memory region
between addresses $\textsf{\&V + i * N}$ and $\textsf{\&V + i * N + M}$.
The identification of such ranges involves the use of a symbolic algebra,
which we have borrowed from the compiler-related literature, as we
explain in Section~\ref{sub:symb}.
Figure~\ref{fig:ex_Regions} also introduces the second challenge that we
tackle:

\begin{Challenge}
\label{ch:cost}
Estimate the profitability of tasks at runtime.
\end{Challenge}

The creation of tasks involves a heavy runtime cost due to allocation,
scheduling and real-time management of the dependence graph
(see Sec.~\ref{sub:adv}).
Ideally, this cost should be paid only for tasks that perform an amount of work
sufficiently large to pay for their management.
Being an interesting program property, on Rice's sense~\cite{Rice53}, the amount
of work performed by a task cannot be discovered statically.
As we show in Section~\ref{sub:profit}, we can try to approximate this quantity, using,
to this end, program symbols, which are replaced with actual values at runtime.
For instance, in Figure~\ref{fig:ex_Regions}, we know that the body of the
innermost loop is formed by five instructions.
Thus, we approximate the amount of work performed by a task with the
expression \textsf{5 * M}.
We use the runtime value of \textsf{M} to determine, during the execution of the
program, if we create a task, or not.
Such test is carried out by the guard at line 7 of the figure, which is part of
OpenMP's syntax. Also, we provide a reliable estimate on the \emph{workload cutoff}
from which a task can be safely spawned without producing performance overhead.
This \emph{cutoff} considers factors such as number of available cores
and runtime information on the task dispatch cost in terms of machine
instructions. This is further explained in Section \ref{sub:profit}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=1\columnwidth]{images/ex_cutoff}
\caption{Challenge~\ref{ch:cutoff}: bounding the creation of recursive tasks with
counter associated with function calls.}
\label{fig:ex_cutoff}
\end{center}
\end{figure}

\begin{Challenge}
\label{ch:cutoff}
Bound the creation of recursive tasks.
\end{Challenge}

We introduce Challenge~\ref{ch:cutoff} by quoting Duran {\em et al.}:
``{\em In task parallel languages, an important factor for achieving a good
performance is the use of a cut-off technique to reduce the number of tasks
created}"~\cite{Duran08b}.
This observation is particularly true in the context of recursive, fine-grain,
tasks, as we analyze in Section~\ref{sub:rec}.
Figure~\ref{fig:ex_cutoff} provides an example.
To place a limit on the number of tasks simultaneously in flight, we associate
the invocation of recursive functions annotated with task pragmas with a
counter -- \textsf{taskminer\_depth\_cutoff} in Figure~\ref{fig:ex_cutoff}.
The guard in line 7 ensures that we never exceed \textsf{DEPTH\_CUTOFF}, a
predetermined threshold.
This example, together with Figure~\ref{fig:ex_Regions}, lets us emphasize that
the code generation algorithms presented in this paper are
parameterized by constants such as \textsf{DEPTH\_CUTOFF}, or
\textsf{WORK\_CUTOFF} in Figure~\ref{fig:ex_Regions}. Although these
have default estimates, we provide them as parameters to be set at will.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=1\columnwidth]{images/ex_privatize}
\caption{Challenge~\ref{ch:privatize}: variable \textsf{j} must be replicated among
tasks, to avoid the occurrence of data races.}
\label{fig:ex_privatize}
\end{center}
\end{figure}

\begin{Challenge}
\label{ch:privatize}
Identify private and shared variables.
\end{Challenge}

The two previous challenges are related to the performance of annotated
programs: if left unsolved, we shall have correct, albeit inefficient programs.
Challenges~\ref{ch:Regions}, and~\ref{ch:privatize}, in turn, are
related to correctness.
Challenge~\ref{ch:privatize} asks for the identification of variables that
must be replicated among threads.
This process of replication is called {\em privatization}.
As an example, variable \textsf{j} in Figure~\ref{fig:ex_privatize} must
be privatized.
In the absence of such action, that variable will be shared among all the
tasks created at line 5 of Figure~\ref{fig:ex_privatize}.
Because \textsf{j} is written within these tasks, race conditions would
ensue.
Section~\ref{sub:variance} explains how we distinguish private from
shared variables.

% The ability to handle pointers through speculative parallelization.

% when applied to general-purpose integer-intensive applications that have complex control flow and excessive pointer accesses, traditional parallelization tech- niques become quite ineffective, as they need to conservatively ensure program correctness by synchronizing all potential dependences in the program

% For example in 473.astar if we ignore dependences that only occur in less than 20% of all iterations, we can parallelize loops that correspond to 96% of total execution.

\section{Solution}
\label{sec:sol}

Figure~\ref{fig:alg_main} provides a high-level view of a source-to-source
compiler that incorporates the techniques discussed in this paper.
This pseudo-code uses several concepts well-known in the compiler's
literature, such as control flow graphs~\cite{Kildall73} and
dependence graphs~\cite{Ferrante87}.
We built on top of such concepts, as well as others, like
symbolic range analysis~\cite{Blume94}, and natural loops~\cite{Allen70}.
In the rest of this section we explain how we have combined this previous
knowledge in order to delimit and annotate tasks in structured programs.
Our presentation focus on the new elements that we had to add onto known
techniques, in order to adapt them to our purposes.

\begin{figure}[b]
\begin{center}
\includegraphics[width=1\columnwidth]{images/alg_main}
\caption{The main steps of our code generator.}
\label{fig:alg_main}
\end{center}
\end{figure}

\paragraph{Tasks: syntax and scope:}
Our goal is to identify {\em tasks} in {\em structured} programs.
Definition~\ref{def:task} formalizes the notion of task.
A structured program is a program that can be partitioned in {\em hammock
regions}, a concept introduced by Ferrante {\em et al.}~\cite{Ferrante87} in
the middle 80's.
The structure of this program is described by a a Control Flow Graph $G$ --
a directed graph with a starting node $s$, and an exit node $x$.
A hammock region is a subgraph $G' \subseteq G$, with a special node
$h$, which {\em dominates}\footnote{A node $n_1 \in G$ dominates another node
$n_2 \in G$ if every path from  $S$ to $n_2$ goes across $n_1$.
Inversely, $n_1$ post-dominates $n_2$ if every path from $n_2$ to $x$ must
go across $n_1$.} every other node in $G'$.
Additionally, there exists a node $w \in G$, $w \notin G'$, such that
$w$ {\em post-dominates} every node in $G'$.
In this definition, $h$ is the {\em entry point}, and $w$ is the {\em exit point}
of $G'$.

\begin{definition}[Task]
\label{def:task}
Given a program $P$, a task $T$ is a tuple $(G', M_i, M_o)$ formed by a hammock
region $G'$, plus a set $M_i$ of memory regions representing data that $T$ reads,
and a set $M_o$ of memory regions representing data that $T$ writes.
\end{definition}

Definition~\ref{def:task} uses the concept of {\em memory region}.
Syntactically, memory regions are described by program variables and/or pointers
plus ranges of dereferensable offsets.
Given two tasks: $T_1 = (G_1, M_{i1}, M_{o1})$ and $T_2 = (G_2, M_{i2}, M_{o2})$,
if $M_{o1} \cap M_{i2} \neq \emptyset$, then we say that $T_2$ {\em depends} on
$T_1$.
If a program $P$ is partitioned into a set of $n$ tasks, then this partition is
said to be {\em correct} if it abides by Definition~\ref{def:correctness}.

\begin{definition}[Correctness]
\label{def:correctness}
A set $T$ of $n$ tasks is a correct parallelization of a program $P$ if:
\begin{compactenum}
\item $T$ does not contain cyclic dependence relations;
\item the execution of the tasks in $T$ in any ordering determined by dependence
relations, leads to the same results as the sequential execution of $P$.
\end{compactenum}
\end{definition}

\begin{example}[Memory Regions]
\label{ex:regions}
Below we have two tasks, $T_{\mathit{foo}} = (\mathsf{foo}, \{\mathsf{v[i - 1]}\},
\{v[i]\})$ and $T_{\mathit{bar}} = (\mathsf{bar}, \{v[i]\}, \{\})$.
Dependencies are identified via the \textsf{depend} clause.
In this example, each SESE region is formed by one function call:
\end{example}

\includegraphics[width=1\columnwidth]{images/ex_depends}

\paragraph{The annotation zoo.}
Our equipment of choice to reveal task parallelism in programs is OpenMP 4.0.
This annotation system provides us with a set of pragmas that indicate to compiler
and runtime environment when and how tasks must be created.
We describe below which annotations we are using, and explain, informally, their
semantics.
Full overviews of the syntax and semantics of these annotations is publicly
available\footnote{For a quick overview, we refer the reader to the leaflet
``Summary of OpenMP 4.0 C/C++ Syntax", which is made available by the OpenMP
group.}; hence, we shall not dive into their details.
%
\begin{compactitem}
\item \textsf{parallel} (\textit{clauses}): forms a team of threads
which will execute the marked program region in parallel.
\item \textsf{single} (\textit{clauses}): specifies that a
program region must be executed by one thread in the team.
\item \textsf{task} (\textit{clauses}): creates a new task.
\item \textsf{taskwait}: defines a synchronization point where
threads must wait for the completion of child tasks.
\end{compactitem}
%
Some pragmas are associated with lists of clauses.
There are several of these clauses available in OpenMP 4.0; however, we only use
the following:
\begin{compactitem}
\item \textsf{default([shared/private])}: indicates that variables are either
shared or replicated among tasks.
\item \textsf{firstprivate($v$)}: indicates that $v$ must be replicated among
tasks, and initialized with its value at the point where the annotation is
executed.
\item \textsf{untied}: if a task has this modifier, then its associated code can
be executed by more than one thread; e.g., threads might alternate execution due
to preemption and load balancing.
\item \textsf{depend}($\mathit{in}/\mathit{out}/\mathit{inout}$):
determine if data is read ($\mathit{in}$), written ($\mathit{out}$) or both
within a task region; thus, effectively setting dependences among tasks.
\item \textsf{if}({\em condition}): this clause defines the conditions
under which a task is allowed to create threads.
We use it to enforce the cost model.
\end{compactitem}

\subsection{Mapping Program Regions to Tasks}
\label{sub:identification}

% Windmills and vanes
A {\em task candidate} is a program region that can run in parallel with the
rest of the program.
To identify task candidates, we rely on the structure of the program's
control flow graph.
To this end, we identify, subgraphs in the CFG called {\em windmills}, a term
coined by Rideau {\em et al.}~\cite{Rideau08} to describe structural relations
between register copies.
We adopt a slightly more general definition than Rideaus'; however, the
graphical shape of our graphs remain similar:

\begin{definition}[Windmill]
\label{def:windmill}
A windmill is a graph $G_w = G_c \cup G_{v1} \cup \ldots \cup G_{vn}$ formed by
a strongly connected component $G_c$, its center, plus $n$ components (not
necessarily strong) $G_{vi}$, the vanes, such that:
\begin{compactenum}
\item for any topological ordering of $G_w$, and nodes $n_c \in G_c$,
$n_v \in G_{vi}$, we have $n_c$ ahead of $n_v$;
\item for any $i$ and $j$, $1 \leq i < j \leq n$, $G_{vi}$ and $G_{vj}$ do not
share vertices.
\end{compactenum}
\end{definition}

Statically, a strongly connected set $S$ in the  \emph{Dependence Graph} $G$ represents cyclic dependence between instructions in any iterative construct.
It arises from a loop in the program or, if $G$ is interprocedural,
from a recursive pattern.

Let $L = \{S_{1}, S_{2}, ..., S_{n}\} \subset W$ be the list of $n$ strongly connected sets contained
in a windmill $W$. We call $S_{b}$ to be a \emph{windmill base} if $\forall S_{i} \subset L$, $S_{b} \geq S_{i}$ in any topological sorting of $G$.

\begin{definition}[Helix]
\label{def:helix}
Given a windmill $W$  and a windmill base $S \subset W$ in the \emph{Dependence Graph} $G$, a set of nodes $H$ is a Helix if, and only if:
\begin{itemize}
\item $H \subset W$;
\item $H  \cap S_{b} = \emptyset$;
\item $H$ is connected and maximal.
\end{itemize}
\end{definition}

We can view a \emph{Helix} as a set of nodes in the \emph{Dependence Graph}
that depends on a strongly connected set which is a \emph{windmill base}.
That means helices are all those subgraphs that depend on
the \emph{windmill base} but do not form a cyclic dependence with it.
Figure \ref{fig:helix} illustrates both concepts.

The insight behind  \emph{Helices} is that they represent instructions that 
are deemed to be executed more than once, for they depend on a cyclic
dependence (\emph{windmill base}), and, since they do not form a strongly
connected set itself, they can be executed in parallel.
 
\begin{figure}[t!]
\begin{center}
\includegraphics[width=1\columnwidth]{images/helix}
\caption{\emph{Windmills} and \emph{Helices} corresponding to the sample code above.
We see the \emph{windmill} 
$W = \{A, B, C, D, E, F, G, H\}$, the windmill base $S_{b} = \{A, B, C\}$ 
and two \emph{helices} $H_{1} = \{H\}$ and $H_{2} = \{D, E, F, G\}$.  Note
that there are more than one strongly connected, but only of them is a
base, which is the one at the top topologically.}
\label{fig:helix}
\end{center}
\end{figure}

Thus, the first step in the algorithm to find tasks is to map these \emph{Helices}
into program regions. The algorithm scans through the \emph{Dependence Graph}
looking for \emph{Helices} and then finds the minimal covering region for each \emph{Helix}.
These minimal regions are later going to be annotated as tasks in the original
program, should they fit a cost model.

After finding potential task regions, the next critical step is the \emph{expansion}.

\begin{definition}[Region expansion]
\label{def:expansion}
A task region $R$ can be expanded into a larger region $R'$ if, and only if:
\begin{itemize}
\item $|M(R')| \leq |M(R)|$, in which $M(R)$ and $M(R')$ are the set of input and output memory
dependences in regions $R$ and $R'$ respectively.
\item $R'$ is still inside a loop or a cyclic construct.
\end{itemize}
\end{definition}

Figure \ref{fig:expand_alg} illustrate the algorithm of expansion. It simply tries to expand while
all the conditions in Definition \ref{def:expansion} are met, and then halts
when they are not. After this process, {\Taskminer} ends up with a list
of potential \emph{task regions}, which will then pass under
a series of operations prior to the annotation to source code, the 
ultimate step.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\columnwidth]{images/expand_alg}
\caption{Task discovery via expansion of hammock regions.}
\label{fig:expand_alg}
\end{center}
\end{figure}


\subsection{Finding Symbolic Bounds of Arrays}
\label{sub:symb}

\pedro{I don't know much of this part. I asked gleison to write me a paragraph and
then I'll review it and try to write something here.}

\subsection{Estimating the Profitability of Tasks}
\label{sub:profit}

\pedro{We need to discuss the THRESHOLD. Are we going to use
César's experiments at Unicamp concerning the different runtimes? 
Because if yes we need to think on how to write it.}

\subsection{Limiting the Creation of Recursive Tasks}
\label{sub:rec}

\subsection{Separating Private from Shared Variables}
\label{sub:variance}

\subsection{Mapping IR onto Source Code}
\label{sub:ir}

\pedro{I asked gleison and he'll sit with me to explain everything ,
how it's done, and then I'll write it here.}

\section{Evaluation}
\label{sec:eval}
% In section 4. “Evaluation", subsection “Performance"; you state the only reason why programmers will strive for parallelism would be to achieve higher performance (faster program execution). I do not fully agree, and I believe we discussed that point earlier. There may be cases where performance of a single big core would be good enough to meet some target schedule (either response time or frame rate, etc.) still you may want to explore parallel versions of the code running on a cluster of smaller cores or a combination of a slow CPU and GPU to achieve better power efficiency.

% I believe this paper could be made stronger by also investigating the impact of power efficiency as a component of the cost function - if one looks at the latest processor chips from Intel and AMD, I can (finally) see there is a trend to increase the number of cores for all platforms.

% In the past there has been not much opportunity to take advantage of parallelism on 2 core platforms (especially notebook designs used to limit the number of real cores), but with 6 or 8 core processors entering the main stream, we may take advantage of running more processors slower (lower voltage) or spreading out the workload over a larger die area. It is quite interesting to note that running one core hot will increase the leakage in that area of the die much more (exponentially with temperature), so distributing work over a larger die area results in reduced leakage - having said that, a similar effect can be achieved by migrating the workload from core to core.

We performed a thorough evaluation on \Taskminer{}'s applicability in real programs. Our test framework is divided in five aspects:

\begin{compactitem}
\item \textsf{Performance}: we run \Taskminer on the BOTS Benchmark and show that the programs annotated by taskminer are sometimes as fast as programs annotated manually, and never slower than their sequential counterparts, thus proving it to be a conservative and useful technique. 
\item \textsf{Optimizations}: we show some optimizations that can be applied to \Taskminer in order to improve performance of the annotated programs.
\item \textsf{Versatility}: we demonstrate \Taskminer's versatility when it is able to find tasks in several different program paradigms.
\item \textsf{Applicability}: we run \Taskminer on larger benchmarks to show the potential of finding task parallelism in big applications.
\item \textsf{Practicality}: here we present \Taskminer's runtime and show that it can be easily used by a programmer to find task parallelism in applications. We evidence \Taskminer's practicality when compared to the method of manually finding parallelism, determining data dependences annotating the program.
\end{compactitem}

%PERFORMANCE
%Here, we show the results for our small benchmarks (toys & bots)
\textbf{\textit{Performance}}. 

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth]{images/Absolute_speedups}
\caption{Speedup comparisons -- automatic annotations compare favourably against
manual interventions, and lead to great performance gains.
The Y-axis shows the speedup of either manual intervention, or automatic
annotation (this paper) onto the original programs.}
\label{fig:Absolute_speedups}
\end{center}
\end{figure}

%OPTIMIZATIONS
%Here, we show the results on two big optimizations: recursion depth and cost model.
%We can use the same benchmarks as above.
\textbf{\textit{Optimizations}}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\columnwidth]{images/Opt_Speedups}
\caption{The benefit of our optimizations.
Each optimization (profitability analysis and recursion bounds) lead to
non-trivial speedups.}
\label{fig:Opt_Speedups}
\end{center}
\end{figure}

%VERSATILITY
%The goal here is to show that TaskMiner is a versatile tool, capable of finding many types of task parallelism in the code. 
%And the types of tasks that are going to be mined can be easily set, should the programmer ever desires to.
\textbf{\textit{Versatility}}.


%APPLICABILITY
%We'll show TaskMiner application on larger benchmarks, such as those of spec or those of the LLVM's test-suite.
\textbf{\textit{Applicability}}.


%PRACTICALITY
%The main objective here is to show that TaskMiner can be easily utilized by a programmer to find tasks in their source code. 
%We'll test TaskMiner's running time.
\textbf{\textit{Practicality}}.


\section{Related Work}
\label{sec:rw}

% Talk about annotations vs libraries, e.g., Wool, Intel TBB. See: "A Comparison of some recent Task-based Parallel Programming Models"

\section{Conclusion}
\label{sec:conc}

\bibliography{references}

\end{document}
